{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0078e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24924c67",
   "metadata": {},
   "source": [
    "### 1. Write the Python code to implement a single neuron. - Frak this Im writin C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc27a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, activation_function, n_in, n_out):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.out_weights = [random.choice([0.001*i for i in range(-1000, 1000)]) for _ in range(n_out)]\n",
    "        self.activation_function = activation_function\n",
    "        self.out = None\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        print(f\"length={len(input)}, {input=}\")\n",
    "        print(f\"{self.n_in=}\")\n",
    "        if len(input) != self.n_in:\n",
    "            raise Exception(\"N inputs does not match definition!\")\n",
    "    \n",
    "        self.out = self.activation_function(sum(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e826a3",
   "metadata": {},
   "source": [
    "### 2. Write the Python code to implement ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d6a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, input : int):\n",
    "        if input > 0:\n",
    "            return input\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c97e1",
   "metadata": {},
   "source": [
    "### 3. Write the Python code for a dense layer in terms of matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f0389",
   "metadata": {},
   "source": [
    "#### As matrix multiplication in math notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2890952",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Out} = \n",
    "\\begin{bmatrix}\n",
    "out_1 \\\\\n",
    "out_2 \\\\\n",
    "\\vdots \\\\\n",
    "out_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{W} = \n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n1} & w_{n2} & \\cdots & w_{nm}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{Out} \\times \\mathbf{W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \n",
    "\\begin{bmatrix}\n",
    "out_1 \\\\\n",
    "out_2 \\\\\n",
    "\\vdots \\\\\n",
    "out_n\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
    "w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n1} & w_{n2} & \\cdots & w_{nm}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Donde Z será la preactivación de la siguiente capa de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cba601",
   "metadata": {},
   "source": [
    "#### In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95aafa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PARAMETERS ###\n",
      "layer_1_size=4\n",
      "layer_2_size=1\n",
      "n_in=2\n",
      "n_hidden=1\n",
      "n_out=4\n",
      "### END PARAMETERS ###\n",
      "Layer 1 neurons!\n",
      "Neuron 1!\n",
      "length=2, input=[-0.795, 0.146]\n",
      "self.n_in=2\n",
      "Neuron 2!\n",
      "length=2, input=[-0.514, 0.422]\n",
      "self.n_in=2\n",
      "Neuron 3!\n",
      "length=2, input=[-0.373, 0.657]\n",
      "self.n_in=2\n",
      "Neuron 4!\n",
      "length=2, input=[0.154, 0.6890000000000001]\n",
      "self.n_in=2\n",
      "Layer 2 neurons!\n",
      "length=4, input=[-0.0, -0.0, -0.09002800000000001, 0.12982200000000002]\n",
      "self.n_in=4\n"
     ]
    }
   ],
   "source": [
    "relu_activation = ReLU()\n",
    "\n",
    "n_in = random.choice([i for i in range(1, 5)])\n",
    "n_hidden = random.choice([i for i in range(1, 5)])\n",
    "n_out = random.choice([i for i in range(1, 5)])\n",
    "layer_1_size = random.choice([i for i in range(1, 5)])\n",
    "layer_2_size = n_hidden\n",
    "print(\"### PARAMETERS ###\")\n",
    "print(f\"{layer_1_size=}\")\n",
    "print(f\"{layer_2_size=}\")\n",
    "print(f\"{n_in=}\")\n",
    "print(f\"{n_hidden=}\")\n",
    "print(f\"{n_out=}\")\n",
    "print(\"### END PARAMETERS ###\")\n",
    "\n",
    "layer_1 = [Neuron(activation_function=relu_activation, n_in=n_in, n_out=n_hidden) for _ in range(layer_1_size)]\n",
    "layer_2 = [Neuron(activation_function=relu_activation, n_in=layer_1_size, n_out=n_out) for _ in range(layer_2_size)]\n",
    "\n",
    "print(f\"Layer 1 neurons!\")\n",
    "for idx, neuron in enumerate(layer_1):\n",
    "    print(f\"Neuron {idx + 1}!\")\n",
    "    random_input = [random.choice([0.001*i for i in range(-1000, 1000)]) for _ in range(n_in)]\n",
    "    neuron(random_input)\n",
    "\n",
    "print(f\"Layer 2 neurons!\")\n",
    "for idx, neuron_2 in enumerate(layer_2):\n",
    "    neuron_2_inp = [neuron_1.out_weights[idx] * neuron_1.out for neuron_1 in layer_1]\n",
    "    neuron_2(neuron_2_inp)\n",
    "\n",
    "# Es lo mismo que arriba pero visto desde otra perspectiva. A fin de cuentas estamos iterando sobre los pesos de salida de la capa previa y multiplicando por esa activación de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cdaa78",
   "metadata": {},
   "source": [
    "### 4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\n",
    "Did at 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1b242",
   "metadata": {},
   "source": [
    "### 5. What is the \"hidden size\" of a layer?\n",
    "Output of the layer. Normally used before the final layer of the neural net since the last output would be the output size already."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632a3ab",
   "metadata": {},
   "source": [
    "### 6. What does the t method do in PyTorch?\n",
    "Transpose the matrix/tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52f548",
   "metadata": {},
   "source": [
    "### 7. Why is matrix multiplication written in plain Python very slow?\n",
    "Because it uses the native Python implementation which is slow when it comes to for loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedbcb67",
   "metadata": {},
   "source": [
    "### 8. In matmul, why is ac==br. That is: given the matmul AxB, why must A's columns be of the same size as B's rows?\n",
    "Well that's how they taught me at school :v.\n",
    "Besides that, We multiply and add the values of the row in the first matrix to each of the values of the column of the other one. If the sizes weren't equal, this operation would not be possible.\n",
    "\n",
    "**From chatGPT**\n",
    "When you multiply two matrices, each element of the resulting matrix is a dot product between a row from matrix A and a column from matrix B.  \n",
    "→ So if c ≠ b, you can’t compute the dot product because the two vectors don’t align in size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3c04b",
   "metadata": {},
   "source": [
    "### 9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "Using %time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acf42b",
   "metadata": {},
   "source": [
    "### 10. What is \"elementwise arithmetic\"?\n",
    "Applying the same operation to all elements in 2 tensors. To do so, both tensors must be of the same rank and dim size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672a914",
   "metadata": {},
   "source": [
    "### 11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4519ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(100, 200)\n",
    "b = torch.randn(100, 200)\n",
    "\n",
    "(a > b).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d00b8",
   "metadata": {},
   "source": [
    "### 12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "It simply is a value like a float. We can convert it to Python simply by casting it like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38f4cd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank0 = torch.tensor(1.)\n",
    "rank0, float(rank0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb921d07",
   "metadata": {},
   "source": [
    "### 13. What does this return, and why? tensor([1,2]) + tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4477f859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2]) + torch.tensor([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198a052",
   "metadata": {},
   "source": [
    "Because since they both have the same rank (1) and one of them has size 1 which makes it broadcastable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3169a",
   "metadata": {},
   "source": [
    "### 14. What does this return, and why? tensor([1,2]) + tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38171d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "torch.tensor([1,2]) + torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201d4ec",
   "metadata": {},
   "source": [
    "Because since they both have the same rank (1) but neither of them has size 1 on said rank, there is no broadcast possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b11f1",
   "metadata": {},
   "source": [
    "### 15. How does elementwise arithmetic help us speed up matmul?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce4fe7",
   "metadata": {},
   "source": [
    "Since the dot product (matrix multiplication) can be expressed as elementwise arithmetic by using 2 vectors, it speeds up the matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ecbe5",
   "metadata": {},
   "source": [
    "### 16. What are the broadcasting rules?\n",
    "1. Each tensor has at least one dimension.\n",
    "2. To be broadcastable, after aligning the dimensions to the right, tensors' dimensions must be either equal, 1 or non existant from right to left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9735a",
   "metadata": {},
   "source": [
    "<!-- ### 17. What is expand_as? Show an example of how it can be used to match the results of broadcasting. -->\n",
    "Expand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size())."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1801ba",
   "metadata": {},
   "source": [
    "### 18. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "By adding a size 1 dimension in specific parts thay may need it for broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b932390",
   "metadata": {},
   "source": [
    "### 19. How can we use indexing to do the same operation as unsqueeze?\n",
    "**De chatGPT** To achieve the same effect as `unsqueeze()` using **indexing**, you can use **`None`** (or `np.newaxis` in NumPy) to add a new dimension. In PyTorch or NumPy, `unsqueeze(dim)` adds a dimension of size 1 at the specified `dim`. Using indexing, you can do the same by inserting `None` in the desired position in the indexing tuple.\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3])  # Shape: [3]\n",
    "\n",
    "# Using unsqueeze\n",
    "x_unsq = x.unsqueeze(0)      # Shape: [1, 3]\n",
    "\n",
    "# Using indexing with None\n",
    "x_indexed = x[None, :]       # Shape: [1, 3]\n",
    "```\n",
    "\n",
    "You can also do:\n",
    "```python\n",
    "x.unsqueeze(1)  # Shape: [3, 1]\n",
    "x[:, None]      # Shape: [3, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc1ee7",
   "metadata": {},
   "source": [
    "### 20. How do we show the actual contents of the memory used for a tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "461d0c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8089497089385986"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(100,200)\n",
    "# x.storage()   # Shows memory used in an array (as it would be stored in memory)\n",
    "x.storage()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd58e48",
   "metadata": {},
   "source": [
    "### 21. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e9d02ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6134, -0.8436,  0.4758])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row\n",
    "vector = torch.randn(3)\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26c41426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6826, -0.5328,  0.0057],\n",
       "        [ 0.6754, -1.0440, -0.1831],\n",
       "        [-0.4084, -0.8969,  0.2440]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = torch.randn(3, 3)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "427651ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2960, -1.3764,  0.4815],\n",
       "        [ 0.0620, -1.8875,  0.2927],\n",
       "        [-1.0218, -1.7405,  0.7198]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector + matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e243c7",
   "metadata": {},
   "source": [
    "#### Even if defined as column vector. For instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b67bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7786, 0.6067, 1.2278]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row\n",
    "vector = torch.randn(1, 3)\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "51932300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0961,  0.0739,  1.2335],\n",
       "        [ 1.4540, -0.4373,  1.0447],\n",
       "        [ 0.3702, -0.2902,  1.4718]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector + matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43745d3f",
   "metadata": {},
   "source": [
    "### 22. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "No because we are getting references to already existant data, which makes learning these rules very useful for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23ac4e",
   "metadata": {},
   "source": [
    "### 23. Implement matmul using Einstein summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5832ea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5719, 0.7141],\n",
      "        [0.4804, 1.4062]])\n",
      "tensor([[ 1.2532, -0.1285, -1.2538],\n",
      "        [-0.1344, -0.8296,  0.2436]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6208, -0.6659, -0.5431],\n",
       "        [ 0.4130, -1.2282, -0.2598]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "print(x)\n",
    "y = torch.randn(2, 3)\n",
    "print(y)\n",
    "\n",
    "torch.einsum(\"ij,jk->ik\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4ee96",
   "metadata": {},
   "source": [
    "### 24. What does a repeated index letter represent on the left-hand side of einsum?\n",
    "Implicit sum over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c98b0d",
   "metadata": {},
   "source": [
    "### 25. What are the three rules of Einstein summation notation? Why?\n",
    "1. Repeated indices on the left side are implicitly summed over if they are not on the right side\n",
    "2. Each index can appear at most twice on the left side\n",
    "3. Unrepeated indices on the left side must appear on the right side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4049081",
   "metadata": {},
   "source": [
    "### 26. What are the forward pass and backward pass of a neural network?\n",
    "Forward es tira' p'alante y backward es tirah p'atrás\n",
    "- Forward: Desde el punto de inicio, calcular la salida de cada capa y en consecución calcular las de las subsecuentes capas.\n",
    "- Backward: Basándonos en una referencia con la que se puede calcular un error, calcular los gradientes desde la última capa hacia la primera que nos servirán para actualizar los parámetros de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07383642",
   "metadata": {},
   "source": [
    "### 27. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "Porque estas se utilizan para el cálculo de los gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4934158f",
   "metadata": {},
   "source": [
    "### 28. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "Que en las consecuentes capas de la red neuronal nos encontraremos con activaciones que se incrementarán exponencialmente, destruyendo la fase de propagación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac9c26",
   "metadata": {},
   "source": [
    "### 29. How can weight initialization help avoid this problem?\n",
    "Utilizando pesos cuya dimensión mantenga las entradas de cada capa de la red neuronal con un std = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86642a14",
   "metadata": {},
   "source": [
    "### 30. What is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU?\n",
    "Teníamos las inicializaciones de Xavier y de Kaimi:\n",
    "#### Xavier init\n",
    "$$\n",
    "\\sqrt\\frac{1}{n}\\\\\n",
    "$$\n",
    "#### Kaiming init\n",
    "$$\n",
    "\\sqrt\\frac{2}{n}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc6429",
   "metadata": {},
   "source": [
    "### 31. Why do we sometimes have to use the squeeze method in loss functions?\n",
    "Porque una dimensión de tamaño 1 no aporta información alguna. Como podría ser en un vector/tensor columna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58564f2a",
   "metadata": {},
   "source": [
    "### 32. What does the argument to the squeeze method do? Why might it be important to include this argument, even though PyTorch does not require it?\n",
    "Squeeze to the specific dimension. By default it will be the first to the right. Might include it just to be sure of where is it squeezin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b97fff",
   "metadata": {},
   "source": [
    "### 33. What is the \"chain rule\"? Show the equation in either of the two forms presented in this chapter.\n",
    "Sheeeesh there's plenty of equations in [notes](./Notes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3de20",
   "metadata": {},
   "source": [
    "### 34. Show how to calculate the gradients of mse(lin(l2, w2, b2), y) using the chain rule.\n",
    "Already in [notes](./Notes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f8457",
   "metadata": {},
   "source": [
    "### 35. What is the gradient of ReLU? Show it in math or code. (You shouldn't need to commit this to memory—try to figure it using your knowledge of the shape of the function.)\n",
    "Already in notes too. Altough we might as well say it is 1 when x > 0, else is 0. It's funny cause it's not differentiable at 0 hehe :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918fad5",
   "metadata": {},
   "source": [
    "### 36. In what order do we need to call the *_grad functions in the backward pass? Why?\n",
    "From the furthest layer to the right to the one of the left. That is from the one that outputs, then the previous, then the previous...  \n",
    "It's like this because we need to compute the gradients based on the gradients of the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee126a",
   "metadata": {},
   "source": [
    "### 37. What is __call__?\n",
    "Special keyword that allows to call the class in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec811af2",
   "metadata": {},
   "source": [
    "### 38. What methods must we implement when writing a torch.autograd.Function?\n",
    "Forward and backward methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0bea3",
   "metadata": {},
   "source": [
    "### 39. Write nn.Linear from scratch, and test it works.\n",
    "Done above using Python :v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53982ef4",
   "metadata": {},
   "source": [
    "### 40. What is the difference between nn.Module and fastai's Module?\n",
    "Basically we do not need to call super()__init__ with fastai Module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
