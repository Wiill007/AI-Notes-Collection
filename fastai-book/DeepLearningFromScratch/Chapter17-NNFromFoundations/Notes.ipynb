{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tensor([10, 20, 30])\n",
    "m = tensor([[1., 2, 3], [4, 5, 6], [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape, m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 22., 33.],\n",
       "        [14., 25., 36.],\n",
       "        [17., 28., 39.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c + m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose this is a batch of RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 256, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch = torch.rand(64, 3, 256, 256)\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize it with vectors of 3 elements. One for the mean and one for std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4998, 0.5001, 0.4998]), torch.Size([3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(image_batch, dim=(0, 2, 3))\n",
    "mean, mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2887, 0.2886, 0.2887]), torch.Size([3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.std(image_batch, dim=(0, 2, 3))\n",
    "std, std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_mean = mean.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "broadcasted_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([[[[0.4998]],\n",
       "\n",
       "         [[0.5001]],\n",
       "\n",
       "         [[0.4998]]]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_mean.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_std = std.unsqueeze(1).unsqueeze(1)\n",
    "broadcasted_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([[[0.2887]],\n",
       "\n",
       "        [[0.2886]],\n",
       "\n",
       "        [[0.2887]]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_std.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_images = (image_batch - broadcasted_mean)/broadcasted_std\n",
    "normalized_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstein Summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compact representation for combining products and sums in a general way. For example:  \n",
    "ik,kj->ij\n",
    "\n",
    "Lefthand side represents the \"operators\" while the right side would be the result's dimensions. The rules of Einsum are:\n",
    "1. Repeated indices on the left side are implicitly summed over if they are not on the right side\n",
    "2. Each index can appear at most twice on the left side\n",
    "3. Unrepeated indices on the left side must appear on the right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1973,  0.2416, -0.6079],\n",
       "         [-0.2403, -0.3139, -0.1490]]),\n",
       " tensor([[-0.8592,  2.0951],\n",
       "         [-0.8479, -0.7700],\n",
       "         [ 1.3391, -0.5981]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "y = torch.randn(3,2)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1973, -0.2403],\n",
       "        [ 0.2416, -0.3139],\n",
       "        [-0.6079, -0.1490]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij->ji\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1885,  0.5909],\n",
       "        [ 0.2732, -0.1727]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ik, kj -> ij', x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4669, -0.0327],\n",
       "        [-0.0327,  0.1785]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, kj -> ik', x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and init a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could stack the 2 layers on top of the other but the result of linear operations is another linear operation. To make it work we should add a nonlinearity in the middle such as the most common used activation function [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x @ w + b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't actually train here, so we only set random valued matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(200, 100)\n",
    "y = torch.randn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(100, 50)\n",
    "b1 = torch.zeros(50)\n",
    "w2 = torch.randn(50, 1)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of our first linear layer would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 50])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = lin(x, w1, b1)\n",
    "l1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But check how the mean and std change **AFTER** the layer. Take into account how torch.randn works:\n",
    "> torch.randn: Returns a tensor filled with random numbers from a normal distribution with mean `0` and variance `1` (also called the standard normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0158), tensor(1.0073), tensor(0.0874), tensor(1.0073))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std(), w2.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2552), tensor(10.1200))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.mean(), l1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are getting 10 times the original std we'd get from the original torch.randn. This is because of the existing addition in matrix multiplication. Basically, we are adding 100 values with mean 0. The mean itself will NOT change, but the spread of the values will be hugely increased after a single layer as seen. A more exaggerated version would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0056), tensor(100.0168))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.randn(10000,10000)\n",
    "test = test @ test\n",
    "test.mean(), test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan]]),\n",
       " tensor(nan),\n",
       " tensor(nan))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200,100)\n",
    "for _ in range(50): x = x @ torch.randn(100,100)\n",
    "x[0:5, 0:5], x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with floats, give it a big enough number and we go to *** xddd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing initialization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Xavier Glorot and Yoshua Bengio's weights init scaling](https://oreil.ly/9tiTC)\n",
    "Basically scale to a given linear layer in order to keep std=1:\n",
    "$$\n",
    "\\sqrt\\frac{1}{n}\\\\\n",
    "$$\n",
    "With *n* being the number of inputs. In our case, with we'd have 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3581,  0.4751, -1.5982,  0.8653, -0.9027],\n",
       "         [-0.6477,  0.9074,  0.3204, -0.3126,  0.0870],\n",
       "         [ 0.9161, -0.0672, -1.3581,  1.2238, -1.2131],\n",
       "         [ 0.1557, -0.3300,  0.1554, -0.0133, -0.2455],\n",
       "         [ 0.2525,  0.3681, -0.5425,  0.3638, -0.2616]]),\n",
       " tensor(0.0025),\n",
       " tensor(0.7898))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200,100)\n",
    "for _ in range(50): x = x @ (0.1 * torch.randn(100,100))\n",
    "x[0:5, 0:5], x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we can see we are keeping the proper std, mean and values. **Notice that even adding 0.01 to the scale factor the std will go up to 100! Sometimes even with the rectification it will change around 30% 1.0 value!!!**\n",
    "> Now test second layer with this initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "x = torch.randn(200,100)\n",
    "w1 = torch.randn(100, 50) / sqrt(100)\n",
    "b1 = torch.zeros(50)\n",
    "w2 = torch.randn(50, 1) / sqrt(50)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0106), tensor(0.9913))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = lin(x, w1, b1)\n",
    "l1.mean(), l1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3896), tensor(0.5751))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = relu(l1)\n",
    "l2.mean(), l2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Corrected init taking ReLU into account](https://oreil.ly/-_quA)\n",
    "We can see that even with the correction, after ReLU we get that both mean and std have moved. In the paper linked, we can see the computation for the proper initialization taking ReLU into account:\n",
    "$$\n",
    "\\sqrt\\frac{2}{n}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3.3958, 0.0000, 0.0000, 0.0000, 4.7809],\n",
       "         [2.0073, 0.1009, 0.0000, 0.0000, 2.5939],\n",
       "         [2.6231, 0.4898, 0.0000, 0.0000, 3.7671],\n",
       "         [3.8565, 0.6812, 0.0000, 0.0000, 5.3409],\n",
       "         [2.8088, 0.0611, 0.0000, 0.0000, 3.6463]]),\n",
       " tensor(1.1807),\n",
       " tensor(1.8094))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200,100)\n",
    "for _ in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\n",
    "x[0:5, 0:5], x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that's better, the book follows with creating the model. But for more normalization steps read [BatchNorm](https://arxiv.org/abs/1502.03167) or even [the following article](https://medium.com/biased-algorithms/batch-normalization-alternatives-layernorm-and-instancenorm-52bdf43624b9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(200,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    l1 = lin(x, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    l3 = lin(l2, w2, b2)\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember MSE (Mean Squared Error):\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() # Squeeze because of that single dim on cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0987)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(out, y)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
