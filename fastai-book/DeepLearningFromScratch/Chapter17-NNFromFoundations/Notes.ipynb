{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tensor([10, 20, 30])\n",
    "m = tensor([[1., 2, 3], [4, 5, 6], [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape, m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 22., 33.],\n",
       "        [14., 25., 36.],\n",
       "        [17., 28., 39.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c + m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose this is a batch of RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 256, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch = torch.rand(64, 3, 256, 256)\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize it with vectors of 3 elements. One for the mean and one for std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4998, 0.5001, 0.5000]), torch.Size([3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(image_batch, dim=(0, 2, 3))\n",
    "mean, mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2887, 0.2887, 0.2887]), torch.Size([3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.std(image_batch, dim=(0, 2, 3))\n",
    "std, std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_mean = mean.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "broadcasted_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([[[[0.4998]],\n",
       "\n",
       "         [[0.5001]],\n",
       "\n",
       "         [[0.5000]]]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_mean.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_std = std.unsqueeze(1).unsqueeze(1)\n",
    "broadcasted_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([[[0.2887]],\n",
       "\n",
       "        [[0.2887]],\n",
       "\n",
       "        [[0.2887]]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_std.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_images = (image_batch - broadcasted_mean)/broadcasted_std\n",
    "normalized_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstein Summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compact representation for combining products and sums in a general way. For example:  \n",
    "ik,kj->ij\n",
    "\n",
    "Lefthand side represents the \"operators\" while the right side would be the result's dimensions. The rules of Einsum are:\n",
    "1. Repeated indices on the left side are implicitly summed over if they are not on the right side\n",
    "2. Each index can appear at most twice on the left side\n",
    "3. Unrepeated indices on the left side must appear on the right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.1307, -0.2858, -1.7678],\n",
       "         [ 0.2907,  0.4672, -1.6520]]),\n",
       " tensor([[ 0.9601,  0.8377],\n",
       "         [ 0.8417, -0.8874],\n",
       "         [-0.3875, -1.0783]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "y = torch.randn(3,2)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1307,  0.2907],\n",
       "        [-0.2858,  0.4672],\n",
       "        [-1.7678, -1.6520]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij->ji\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6411,  1.2128],\n",
       "        [ 1.3124,  1.6104]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ik, kj -> ij', x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.4852, 2.4583],\n",
       "        [2.4583, 3.0320]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ij, kj -> ik', x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and init a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could stack the 2 layers on top of the other but the result of linear operations is another linear operation. To make it work we should add a nonlinearity in the middle such as the most common used activation function [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x @ w + b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't actually train here, so we only set random valued matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(200, 100)\n",
    "y = torch.randn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(100, 50)\n",
    "b1 = torch.zeros(50)\n",
    "w2 = torch.randn(50, 1)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of our first linear layer would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 50])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = lin(x, w1, b1)\n",
    "l1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But check how the mean and std change **AFTER** the layer. Take into account how torch.randn works:\n",
    "> torch.randn: Returns a tensor filled with random numbers from a normal distribution with mean *0* and variance *1* (also called the standard normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0050), tensor(1.0085), tensor(0.0894), tensor(1.0085))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std(), w2.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0242), tensor(10.0081))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.mean(), l1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are getting 10 times the original std we'd get from the original torch.randn. This is because of the existing addition in matrix multiplication. Basically, we are adding 100 values with mean 0. The mean itself will NOT change, but the spread of the values will be hugely increased after a single layer as seen. A more exaggerated version would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0030), tensor(100.0119))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.randn(10000,10000)\n",
    "test = test @ test\n",
    "test.mean(), test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan]]),\n",
       " tensor(nan),\n",
       " tensor(nan))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200,100)\n",
    "for _ in range(50): x = x @ torch.randn(100,100)\n",
    "x[0:5, 0:5], x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with floats, give it a big enough number and we go to *** xddd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing initialization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Xavier Glorot and Yoshua Bengio's weights init scaling](https://oreil.ly/9tiTC)\n",
    "Basically scale to a given linear layer in order to keep std=1:\n",
    "$$\n",
    "\\sqrt\\frac{1}{n}\\\\\n",
    "$$\n",
    "With *n* being the number of inputs. In our case, with we'd have 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3081, -1.0823, -1.0933, -0.5763, -0.6330],\n",
       "         [-0.6660,  0.4010, -0.3800, -0.1797, -0.2145],\n",
       "         [-0.4112, -0.7967, -0.7344, -0.6401, -0.5505],\n",
       "         [ 0.4466,  1.1501,  0.7380,  0.5851,  0.1092],\n",
       "         [ 0.3216, -0.2872, -0.2907,  0.0939, -0.1568]]),\n",
       " tensor(-0.0017),\n",
       " tensor(0.8171))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200,100)\n",
    "for _ in range(50): x = x @ (0.1 * torch.randn(100,100))\n",
    "x[0:5, 0:5], x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we can see we are keeping the proper std, mean and values. **Notice that even adding 0.01 to the scale factor the std will go up to 100! Sometimes even with the rectification it will change around 30% 1.0 value!!!**\n",
    "> Now test second layer with this initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "x = torch.randn(200,100)\n",
    "w1 = torch.randn(100, 50) / sqrt(100)\n",
    "b1 = torch.zeros(50)\n",
    "w2 = torch.randn(50, 1) / sqrt(50)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0016), tensor(1.0020))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = lin(x, w1, b1)\n",
    "l1.mean(), l1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3993), tensor(0.5838))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = relu(l1)\n",
    "l2.mean(), l2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Corrected init taking ReLU into account](https://oreil.ly/-_quA)\n",
    "We can see that even with the correction, after ReLU we get that both mean and std have moved. In the paper linked, we can see the computation for the proper initialization taking ReLU into account:\n",
    "$$\n",
    "\\sqrt\\frac{2}{n}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.1243, 0.0000, 0.0000, 0.1398],\n",
       "         [0.0000, 0.2184, 0.0000, 0.0000, 0.1575],\n",
       "         [0.0000, 0.2943, 0.0000, 0.0000, 0.2708],\n",
       "         [0.0000, 0.3140, 0.0000, 0.0000, 0.1902],\n",
       "         [0.0000, 0.2908, 0.0000, 0.0000, 0.1748]]),\n",
       " tensor(0.3451),\n",
       " tensor(0.4868))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(200,100)\n",
    "for _ in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\n",
    "x[0:5, 0:5], x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that's better, the book follows with creating the model. But for more normalization steps read [BatchNorm](https://arxiv.org/abs/1502.03167) or even [the following article](https://medium.com/biased-algorithms/batch-normalization-alternatives-layernorm-and-instancenorm-52bdf43624b9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(200,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    l1 = lin(x, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    l3 = lin(l2, w2, b2)\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember MSE (Mean Squared Error):\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() # Squeeze because of that single dim on cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4403)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(out, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla de la cadena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicación\n",
    "\n",
    "La regla de la cadena es una fórmula utilizada para calcular la derivada de una función compuesta. Si tenemos dos funciones, *f(x)* y *g(x)*, la derivada de su composición *f(g(x))* se puede calcular mediante la regla de la cadena. La fórmula general es:\n",
    "\n",
    "$$\n",
    "   \\frac{d}{dx} \\left[ f(g(x)) \\right] = f'(g(x)) \\cdot g'(x)\n",
    "$$\n",
    "Donde:\n",
    "-  *f'(g(x))* es la derivada de la función externa, evaluada en *g(x)*.\n",
    "- *g'(x)* es la derivada de la función interna *g(x)* respecto de *x*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Si ***f(x) = sin(x)* y *g(x) = x^2***, la derivada de la función compuesta ***f(g(x)) = sin(x^2)*** se obtiene aplicando la regla de la cadena:\n",
    "\n",
    "$$\n",
    "   \\frac{d}{dx} \\left[ \\sin(x^2) \\right] = \\cos(x^2) \\cdot 2x\n",
    "$$\n",
    "\n",
    "En este caso:\n",
    "- La derivada de la función externa *f'(x) = cos(x)*, evaluada en *x^2*, es *cos(x^2)*.\n",
    "- La derivada de la función interna *g'(x) = 2x*.\n",
    "\n",
    "Entonces, la derivada de la composición es *2x · cos(x^2)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre su uso\n",
    "La regla de la cadena se usa constantemente para calcular los gradientes en una red neuronal. Esto se debe a que, en sí mismo, ***el gradiente es el resultado de la derivada parcial con respecto a cada peso.*** La forma en la que se usa este gradiente para minimizar el error de la red neuronal mediante el descenso por gradiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descenso por gradiente:**  \n",
    "Se actualiza cada peso mediante la sustracción de el learning rate multiplicado por el gradiente\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "w_{\\text{new}} \\text{is the updated weight.}\\\\\\\n",
    "w_{\\text{old}} \\text{is the current weight.}\\\\\\\n",
    "\\eta \\ \\text{is the **learning rate** (a hyperparameter that controls how much we adjust the weights).}\\\\\\\n",
    "\\frac{\\partial L}{\\partial w} \\ \\text{is the gradient, i.e., the partial derivative of the loss function with respect to the weight.}\\\\\\\n",
    "$$\n",
    "\n",
    "En resumen:\n",
    "- El **gradiente** dice en qué dirección debe moverse el peso.\n",
    "- El **learning rate** controla el tamaño de cada paso.\n",
    "\n",
    "Nota:\n",
    "- En aplicaciones más avanzadas, se puede incorporar conceptos como momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En este caso particular:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cita libro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule tells us that we have:\n",
    "$$\\frac{\\text{d} loss}{\\text{d} b_{2}} = \\frac{\\text{d} loss}{\\text{d} out} \\times \\frac{\\text{d} out}{\\text{d} b_{2}} = \\frac{\\text{d}}{\\text{d} out} mse(out, y) \\times \\frac{\\text{d}}{\\text{d} b_{2}} lin(l_{2}, w_{2}, b_{2})$$\n",
    "\n",
    "Básicamente indicándonos que tenemos que hacer la derivada del loss con respecto a la salida de la red neuronal, es decir, la derivada de mse con respecto a las activaciones finales y luego la derivada de estas activaciones con respecto a los biases que añadimos en la segunda capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicado a nuestro caso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La derivada del MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "$$\\text{Where:}\\\\\\ - \\hat{y_i} \\text{: Es el objetivo real}\\\\\\ - y \\text{: Es el resultado obtenido de la red neuronal}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos que:$$z = y_i - \\hat{y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivada utilizando la regla de la cadena dada esa aplicación sería:\n",
    "$$\\Delta{y_i} = \\frac{dloss}{dy_i} = \\frac{d\\text{MSE}}{dy_i} = \\frac{d\\text{MSE}}{dz}·\\frac{dz}{dy_i}$$\n",
    "$$\\frac{dloss}{dy_i} = \\frac{d\\text{MSE}}{dy_i} = \\frac{1}{n}·2z·\\frac{dz}{dy_i}$$\n",
    "$$\\frac{d\\text{MSE}}{dy_i} = \\frac{2z}{n}·\\frac{dz}{dy_i}$$\n",
    "$$\\frac{d\\text{MSE}}{dy_i} = \\frac{2y_i}{n}$$\n",
    "**Expresada en código:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recordemos: def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() # Squeeze because of that single dim on cols\n",
    "# También, la salida de la capa es un tensor de dimensiones (x, 1) donde x es el tamaño de la capa\n",
    "def mse_grad(layer_output, target):\n",
    "    return (2 / layer_output.shape[0]) * (layer_output.squeeze() - target).unsqueeze(-1) # En realidad se usa ese unsqueeze(-1) al final para mentener la forma de columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La derivada parcial para la última capa (ReLu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU y su derivada:**\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "$$\n",
    "\\frac{dReLu(x)}{dx} = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivada parcial respecto al peso de la capa ReLu:**\n",
    "$$\\Delta{y_i} = \\frac{dloss}{dy_i} = \\frac{d\\text{MSE}}{dy_i} = \\frac{2y_i}{n}$$\n",
    "$$\\frac{dloss}{dw^{[ReLu]}_j} = \\Delta{y_i} · \\frac{dReLu(w^{[ReLu]}_j)}{dw^{[ReLu]}_j}$$\n",
    "$$\n",
    "\\frac{dloss}{dw^{[ReLu]}_j} = \\Delta{y_i} · \n",
    "\\begin{cases} \n",
    "1 & \\text{if } w^{[ReLu]}_j > 0 \\\\\n",
    "0 & \\text{if } w^{[ReLu]}_j \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "**En código:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    # Cuando se hace un tensor > int se aplica el booleano al tensor completo, devolviendo un tensor de ese tamaño. Cuando se hace .float() se devuelve 1.0 o 0.0\n",
    "    # Nos interesa utilizar el gradiente que ya hemos calculado para el resultado, porque en caso contrario tendríamos que usar la fórmula entera para calcular el gradiente de esta capa incluyendo el gradiente de la final\n",
    "    inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La derivada parcial para la primera capa (Multiplicación matricial)\n",
    "[Para entender de donde viene esta derivada](https://math.stackexchange.com/questions/1866757/not-understanding-derivative-of-a-matrix-matrix-product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = inp.t() @ out.g\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # Forward pass\n",
    "    l1 = inp @ w1 + b1\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w2 + b2\n",
    "    # We don't actually need the loss in backward!\n",
    "    loss = mse(out, targ)\n",
    "\n",
    "    # Backward pass\n",
    "    mse_grad(out, targ)\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    relu_grad(l1, l2)\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring model in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor crear clases para las funciones de activación para llamar a los forward y backward passes. Como tienen muchas cosas en común se puede hacer que hereden de una clase padre que bien podría llamarse FuncionCapa o LayerFunction\n",
    "> Nota: La keyword **__call__** en Python te sirve para hacer que esa clase pueda ser llamada. Por ejemplo:\n",
    "```python\n",
    "class myClass():\n",
    "    __init__(self):\n",
    "        pass\n",
    "\n",
    "    __call__(self, args):\n",
    "        # Do somethin\n",
    "        pass\n",
    "\n",
    "x = myClass()\n",
    "results = x(args)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a refactorizar de golpe a Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Pytorch, debes elegir lo qué guardar en una variable de contexto. Así se aseguran de que no guardamos nada innecesario. Además, **return debe devolver los gradientes en la función backward**. A continuación cómo escribiríamos esta red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class MyRelu(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.clamp_min(0.)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i, = ctx.saved_tensors\n",
    "        return grad_output * (i>0).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la forma en la que escribiríamos activaciones completamente personalizadas. A continuación, se muestra la estructura que usarías para poder usar estos hijos de *Function*. Para ello, se debe crear una clase que herede de **nn.Module.** Esta es la estructura general con la cual podríamos definir cualquier modelo personalizado. Para implementarlo se debe:\n",
    "1. Asegurar que superclass __init__ se llama lo primero al inicializarlo.\n",
    "2. Definir cualquier parámetro del modelo como atributo con nn.Parameter.\n",
    "3. Definir una función *forward* que retorne la salida del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_out))\n",
    "    \n",
    "    def forward(self, x): return x @ self.weight.t() + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo modelo completo en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n",
    "        self.loss = mse\n",
    "        \n",
    "    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
