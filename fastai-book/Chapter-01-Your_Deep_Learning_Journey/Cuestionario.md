# Intro test

## Do you need these for deep learning?
- Lots of math F
- Lots of data F
- Lots of expensive computers F
- A PhD F

## Name five areas where deep learning is now the best in the world.
Computer vision, biology, nlp, medicine, playing games...

## What was the name of the first device that was based on the principle of the artificial neuron?
Mark I Perceptron built by Frank Rosenblatt

## Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?
- A set of processing units
- A state of activation
- An output function for each unit
- A pattern of connectivity among units
- A propagation rule for propagating patterns of activities through the network of connectivities
- An activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit
- A learning rule whereby patterns of connectivity are modified by experience
- An environment within which the system must operate

## What were the two theoretical misunderstandings that held back the field of neural networks?
Mainly thinking that they weren't able to compute XOR operations (problem fixed with more neurons this insight was not recognized by the community) and thinking that these amount of layers would make them too slow to be useful.

## What is a GPU?
> Graphics Processing Unit (GPU): Also known as a graphics card. A special kind of processor in your computer that can handle thousands of single tasks at the same time, especially designed for displaying 3D environments on a computer for playing games. These same basic tasks are very similar to what neural networks do, such that GPUs can run neural networks hundreds of times faster than regular CPUs. All modern computers contain a GPU, but few contain the right kind of GPU necessary for deep learning.

## Why is it hard to use a traditional computer program to recognize images in a photo?
Because feature extraction can be a rather complex task. With deep learning, the neural net extracts those features by itself.

## What did Samuel mean by "weight assignment"?
I am guessing he referred to the weights of the neurons in the neural net.
> “weight assignment” refers to the current values of the model parameters.

## What term do we normally use in deep learning for what Samuel called "weights"?
Parameters.

## Draw a picture that summarizes Samuel's view of a machine learning model.
> *Drawn in notebook*

## Why is it hard to understand why a deep learning model makes a particular prediction?
Because we can understand the procedure used to get the weights that make it choose this prediction but we do not know exactly how these weights affect this outcome since they are too many and the net is too big.

## What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?
A mathematical proof called the universal approximation theorem shows that this function can solve any problem to any level of accuracy, in theory.

## What do you need in order to train a model?
Depending on the problem some data and a GPU.
> You will need an architecture for the given problem. You will need data to input to your model. For most use-cases of deep learning, you will need labels for your data to compare your model predictions to. You will need a loss function that will quantitatively measure the performance of your model. And you need a way to update the parameters of the model in order to improve its performance (this is known as an optimizer).

## How could a feedback loop impact the rollout of a predictive policing model?
By making the data being currently generated by using the model biased based on the previous data. The more this biased model is used, more biased data will be generated.
Arrests predicted in specific place by our  -> Law officers attend this place more -> More arrests are done in this place -> New data is biased because of the previous model.

## Do we always have to use 224×224-pixel images with the cat recognition model?
No. We can do any size images but generally we will be resizing all of them.

## What is the difference between classification and regression?
Classification models will return the probability of each class or at least, the last layer neurons will be used to do so using something like softmax. On the other side, regression models will predict one or more numeric quantities.

## What is a validation set? What is a test set? Why do we need them?
Validation set: Used to check the performance of the model. This is data the model must not see during training.
Test set: Last set that **must be reserved apart from the model and even from ourselves** used to evaluate the model at the very end of our efforts. This set is used in order to prevent the model overfitting the validation data through human trial and error.
> The validation set is the portion of the dataset that is not used for training the model, but for evaluating the model during training, in order to prevent overfitting. This ensures that the model performance is not due to “cheating” or memorization of the dataset, but rather because it learns the appropriate features to use for prediction. However, it is possible that we overfit the validation data as well. This is because the human modeler is also part of the training process, adjusting hyperparameters (see question 32 for definition) and training procedures according to the validation performance. Therefore, another unseen portion of the dataset, the test set, is used for final evaluation of the model. This splitting of the dataset is necessary to ensure that the model generalizes to unseen data.

## What will fastai do if you don't provide a validation set?
By default it will use 20% of the data for validation. At least that's the default value of `valid_pct`

## Can we always use a random sample for a validation set? Why or why not?
> A good validation or test set should be representative of new data you will see in the future. Sometimes this isn’t true if a random sample is used. For example, for time series data, selecting sets randomly does not make sense. Instead, defining different time periods for the train, validation, and test set is a better approach.

## What is overfitting? Provide an example.
Model starts getting bad at predicting unknown examples because of overtraining on the same training set.
> Overfitting is the most challenging issue when it comes to training machine learning models. Overfitting refers to when the model fits too closely to a limited set of data but does not generalize well to unseen data. This is especially important when it comes to neural networks, because neural networks can potentially “memorize” the dataset that the model was trained on, and will perform abysmally on unseen data because it didn’t “memorize” the ground truth values for that data. This is why a proper validation framework is needed by splitting the data into training, validation, and test sets.

## What is a metric? How does it differ from "loss"?
Metric will be a measure of how well the model performs based on different criteria dependant on human consumption. Loss will also measure how well the model performs but rather it'll be used with SGD in order to be able to automatically update weights during training.

## How can pretrained models help?
By being models already pretrained on other tasks, those can be used and fine tuned for another more specific or complex task. We can adapt a model that classifies for example plants vs non plants in order to detect a specific type of plant.

## What is the "head" of a model?
Last layer of a model dependant on the training it had before. When fine tuning a pretrained model, this gets replaced by a new layer or layers of the appropiate size for the dataset we are working with.

## What kinds of features do the early layers of a CNN find? How about the later layers?
In this case it focused on diagonal properties and color gradients with specific directions. Later layers would detect specific shapes instead.
> Earlier layers learn simple features like diagonal, horizontal, and vertical edges. Later layers learn more advanced features like car wheels, flower petals, and even outlines of animals.

## Are image models only useful for photos?
Nope. Biomedicine, medicine, social nets... Besides, images can represent other kind of information too like sounds.

## What is an "architecture"?
*"It's something that academics love to talk about, but in practice it is unlikely to be something you need to spend much time on."*
> The architecture is the template or structure of the model we are trying to fit. It defines the mathematical model we are trying to fit.

## What is segmentation?
Pixelwise division of an image in parts based on what object/surface they are in.

## What is y_range used for? When do we need it?
> y_range is being used to limit the values predicted when our problem is focused on predicting a numeric value in a given range (ex: predicting movie ratings, range of 0.5-5).

## What are "hyperparameters"?
Parameters that describe *how the model is trained*. For example architecture, learning rates, data augmentation strategies...

## What's the best way to avoid failures when using AI in an organization?
> Key things to consider when using AI in an organization:
> 1. Make sure a training, validation, and testing set is defined properly in order to evaluate the model in an appropriate manner.
> 2. Try out a simple baseline, which future models should hopefully beat. Or even this simple baseline may be enough in some cases.


