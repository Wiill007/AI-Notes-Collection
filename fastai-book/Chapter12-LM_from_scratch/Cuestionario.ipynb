{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: [Other answers from](https://forums.fast.ai/t/fastbook-chapter-12-questionnaire-wiki/70516)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: If the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd take a subset of the bieg dataset for testing at first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Why do we concatenate the documents in our dataset before creating a language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it helps to the language models understanding of the bos token and eos token. The \"hidden state\", \"cell state\" or \"attention layer\" should forget the text before bos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Other answer: To create a continuous stream of input/target words, to be able to split it up in batches of significant size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: To use a standard fully connected network to predict the fourth word given the previous three words, what tweaks do we need to make to our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since what we are doing with the hidden state and the input embedding is simply stacking both in one bieg tensor, what we can do is stacking 3 input embeddings too instead. To do so we could simply apply a stacking of the 4 tensors instead of just the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: How can we share a weight matrix across multiple layers in PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Other answer: Define one layer in the PyTorch model class, and use them multiple times in the forward method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Write a module that predicts the third word given the previous two words of a sentence, without peeking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel8(Module):\n",
    "    def __init__(self, num_embeddings, num_hidden, num_rnn_layers):\n",
    "        self.embed_layer = nn.Embedding(num_embeddings=num_embeddings)\n",
    "        self.embed2hidden = nn.RNN(input_size=2, hidden_size=num_hidden, num_layers=num_rnn_layers, nonlinearity='tanh')\n",
    "        self.output_layer = nn.Linear(num_hidden, num_embeddings)\n",
    "        pass\n",
    "\n",
    "    def forward(self, word1, word2):\n",
    "        word1_embedding =  self.embed_layer(word1)\n",
    "        word2_embedding = self.embed_layer(word2)\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embed_layer = nn.Embedding(num_embeddings=500, embedding_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: What is a recurrent neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a neural network in which there are several layers in which each input of those layers is the output of the previous one + the embedding of certain input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: What is hidden state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a specific tensor/array/vector that allows the neural net to remember previous iterations. It can be useful for long sequences in which the output of a specific sequence is related to the previous output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: What is the equivalent of hidden state in LMModel1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: To maintain the state in an RNN, why is it important to pass the text to the model in order?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the state helps the predictions based on previous states, if the text is unordered, this would make the hidden state noisy remembering random orders which would make the RNN worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: What is an *\"unrolled\"* representation of an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schematic of the RNN in which all the layers are represented. Including all those repetitive patterns like the RNN itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: Why can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with maintaining the hidden state in long sequences is the derivations needed to be able to compute the gradients during backpropagation. Since at number 10,000 we'd have to recompute 10,000 derivatives, it is not desirable nor interesting to keep such operations during training.\n",
    "\n",
    "In order to fix it, we must use the *\".detach()\"* method from pytorch which will forget older gradients and just focus on the current sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12:What is BPTT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Through Time is the concept of performing backprop **only** through the seen sequences instead of the whole stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13: Write code to print out the first few batches of the validation set, including converting the token IDs back into English strings, as we showed for batches or IMDb data in Chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14: What does the ModelResetter callback do? Why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses the reset method of the model at every callback. We may need it for example if we train using 2 uncorrelated datasets and we don't need the hidden state to remember anything about the previous dataset once we switch to the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15: What are the downsides of predicting just one output word for each three input words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Waste of training steps\n",
    "- Waste of meaningful context if the sequence length is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16: Why do we need a custom loss function for LMModel4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it predicts a word after every input word which will make it have outputs of size seq_length for each input series. Since this is predicting several words, we need to adapt the loss to this multiword prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17: Why is the training of LMModel4 unstable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because larger networks suffer from exploding or disappearing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18: In the unrolled representation, we can see that a recurrent neural network has many layers. So why do we need to stack RNNs to get better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are effectively creating a deeper network which will get to learn richer and deeper features **Not sure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19: Drawing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q20: Why should we get better results in a RNN if we call detach less often? Why might this not happen in practice with a simple RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we would be backproping from a longer horizon of events which would improve the RNN's hidden state memories. With a simple RNN, this memories aren't that many so it would not have that much of an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q21: Why can a deep network result in very large or very small activations? Why does this matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have effectively a sequence of matrix multiplications. This makes the big numbers tend to infinite while those close to 0 get closer and closer to 0. This large or small activations make some neurons effectively \"stupid\" since those are not really used for the computation towards the goal. Also, depending on the nonlinearity it can make many neurons output the same because of the large or small activation. For example, the limit with tanh will be 1 when those weights are very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q22: In a computer's floating-point representation of numbers, which are the most precise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those closer to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q23: Why do vanishing gradients prevent training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because those neurons with a \"vanished\" gradient will no longer be \"alive\" since those will not be able to learn anything making our net effectively more stupid because of such loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q24: Why does it help to have two hidden states in the LSTM architecture? What is the purpose of each one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps with memory management. The *hidden state* helps with focusing on the next token to predict while the *cell state* will help with long short term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q25: What are these two states called in an LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell state and hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q26: What is tanh and how is it related to sigmoid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically a sigmoid centered on 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q27: What is the purpose of this code in LSTMCell:\n",
    "```python\n",
    "h = torch.cat([h, input], dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just stacking the input with the hidden state for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q28: What does chunk do in Pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits a Tensor in equal sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q29: Study the refactores version of LSTMCell carefully to ensure you understand how and why it does the same thing as the nonrefactored version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q30: Why can we use a higher learning rate for LMModel6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clue brother. I guess because we have a deeper net? I mean it's kinda unrelated though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q31: What are the three regularization techniques used in AWD-LSTM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout, activation regularization and temporal activation regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q32: What is dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly setting the output of specific neurons to 0. Normally using some probability *p*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q33: Why do we scale the activations with dropout? Is this applied during training, inference or both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the sum of the dropout layer will result in lower averages than that of a non dropped output. This is only applied during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q34: What is the purpose of the line from Dropout:\n",
    "```python\n",
    "if not self.training: return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid droput during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q35: Experiment with bernoulli_to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q36: How do you set your model in training mode in PyTorch? In evaluation mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q37: Write the equation for activation regularization (in math or code, as you prefer). How is it different from weight decay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q38: Write the equation for temporal activation regularization (in math or code, as you prefer). Why wouldn't we use this for computer vision problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q39: What is weight tying in a LM?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
