# Training a new tokenizer from an old one

> [Anterior](./1.Intro.md) |  | [Siguiente](./2.Train_new_tokenizer.md)

Es de interés entrenar un tokenizador nuevo si deseamos usar un lenguaje diferente en el cual el modelo que nos gustaría usar no está entrenado. Lo más seguro es que tendríamos que entrenar un modelo desde 0 usando este nuevo tokenizador. *Inviable* debido al coste. Sin embargo, vamos a echarle un vistazo.
Un tokenizador no será muy útil si existen nuevas diferencias como las siguientes:
- Nuevo idioma
- Nuevos caracteres
- Nuevo dominio
- Nuevo estilo
> El problema termina siendo que un tokenizador inadaptado genera demasiados tokens y encima algunos son unknown tokens lo cual empeora mucho la performance de nuestro modelo.

El proceso de entrenamiento de un nuevo tokenizador es el siguiente:
1. Buscar un corpus de texto
2. Elegir arquitectura
3. Entrenar tokenizador en el cuerpo
4. Guardar resultado

> ⚠️ Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch. It’s randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm. It’s deterministic, meaning you always get the same results when training with the same algorithm on the same corpus.

## "Pseudocode" for training tokenizer
We are only missing the function `get_training_corpus()`
```python
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
training_corpus = get_training_corpus()
new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
new_tokenizer.save_pretrained("random_name")
```