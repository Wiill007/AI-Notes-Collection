# Putting all together

> [Anterior](./4.%20Multiple_Sequences.md) |  | [Siguiente](../3.%20Fine_Tuning/1.%20Processing_data.md)

Hemos podido ver cómo el API de transformers puede hacer mucho trabajo mediante una función de alto nivel en la que profundizaremos ahora. Cuando llamamos al tokenizador directamente en la oración, recibimos de vuelta unos inputs preparados para ser enviados al modelo.
```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```
En este caso model_inputs contiene todo lo necesario para que el modelo funcione correctamente. Para DistilBERT eso incluye input IDs y attention mask. Otros modelos que aceptan inputs adicionales obtendrán lo que necesiten también en model_inputs.

Es una API de alto nivel muy potente porque puede tokenizar tanto secuencias sencillas como múltiples a la vez sin cambios en el API.
```python
sequence = "I've been waiting for a HuggingFace course my whole life."
model_inputs = tokenizer(sequence)

sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]
model_inputs = tokenizer(sequences)
```

Puede también hacer padding de acuerdo a ciertos objetivos o hacer truncamiento.
```python
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

El Tokenizador también puede convertir los resultados a tensores específicos de cada framework. Por ejemplo *"pt" returns PyTorch tensors, "tf" returns TensorFlow tensors, and "np" returns NumPy arrays:*

```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```


## Special tokens
Si echamos un vistazo a los input IDs devueltos por el tokenizador, veremos que son un poco diferentes a lo que teníamos antes:
```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```
    [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
    [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]

Se han añadido tokens al principio y al final. Si los inspeccionamos:
```python
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```
    "[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
    "i've been waiting for a huggingface course my whole life"

El tokenizador ha añadido los tags CLD y SEP porque el modelo fue preentrenado con ellas. Para obtener los mismos resultados en inferencia necesitaremos añadirlas también.


## Wrapping up: From tokenizer to model
Ejemplo final de cómo el tokenizador puede encargarse del padding, secuencias largas y múltiples tipos de tensores con el API principal.
```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```


