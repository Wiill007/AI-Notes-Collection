# Handling multiple sequences

> [Anterior](./3.%20Tokenizers.md) |  | [Siguiente](./5.%20Putting_all_together.md)

1. How do we handle multiple sequences?
2. How do we handle multiple sequences of different length?
3. Are vocabulary indices the only inputs that allow a model to work well?
4. Is there such a thing as too long a sequence?

En general las oraciones que queramos juntar en un batch van a tener diferentes longitudes, ocasionando errores porque los tensores no se pueden construir con listas de tama√±os diferentes.
Por ello, se suele hacer padding de las frases m√°s peque√±as para que sus longitudes sean iguales a la de la mayor frase.

Cuando hacemos este padding podemos obtener diferentes resultados del tokenizador al hacer dicho padding. Para prevenir esto, hay que indicar a las capas de atenci√≥n que ignoren el token especial de padding. 


## Models expect a batch of inputs
```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)
```
    IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)

Esto ha fallado porque se ha enviado una sola secuencia al modelo mientras ü§ó Transformers models esperan varias frases por defecto. Se ha intentado hacer lo que se hizo en [behind the scenes](./1.%20Behind_the_pipeline.md) pero en realidad, el tokenizador no solamente ha convertido las input_IDs a tensor, si no que tambi√©n a√±ade una dimensi√≥n.

```python
tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])
```
    tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])


A continuaci√≥n ejemplo a√±adiendo una dimensi√≥n:
```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)
```
    Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
    Logits: [[-2.7276,  2.8789]]

**Batching** es el hecho de enviar varias frases a trav√©s del modelo, todas de una vez.


## Padding inputs
La siguiente lista no se puede convertir a tensor porque necesita padding:
```python
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

Para poder funcionar a pesar de ello, usaremos padding para darle al tensor una forma rectangular.
```python
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

Usar esto de golpe tiene el siguiente problema:
```python
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
```
    tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
    tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
    tensor([[ 1.5694, -1.3895],
            [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)

Y es que los resultados al hacer padding cambian. Esto se puede evitar si usamos m√°scaras de atenci√≥n.

*"This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."*

## Attention masks
*"Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: **1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to** (i.e., they should be ignored by the attention layers of the model)."*

El anterior ejemplo con una m√°scara de estas:
```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)
```
    tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)

Obteniendo esta vez los mismos logits para la segunda frase en el batch

## Longer sequences
*With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:*

- Use a model with a longer supported sequence length.
- Truncate your sequences.

*Models have different supported sequence lengths, and some specialize in handling very long sequences. Longformer is one example, and another is LED. If you‚Äôre working on a task that requires very long sequences, we recommend you take a look at those models.*

*Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:*

    sequence = sequence[:max_sequence_length]



