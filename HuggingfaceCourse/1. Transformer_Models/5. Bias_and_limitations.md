# Bias and limitations

> [Anterior](./4.%20SequenceSequence_models.md) |  | [Siguiente](../2.%20Using_Transformers/1.%20Behind_the_pipeline.md)

Una de las mayores limitaciones de utilizar cualquiera de estos modelos preentrenados es que los dataset pueden no tener mucha calidad. Los investigadores tratan de obtener los dataset de la forma m√°s r√°pida posible debido a la naturaleza de crear uno naturalmente (costoso en tiempo). En consecuencia, muchas veces se toma tanto lo mejor como lo peor de internet. 

Se puede ver un ejemplo claro utilizando BERT para fill-mask pipeline:
```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])

['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
```

Al responder a esta pregunta, el modelo da **solamente una respuesta no influida por el g√©nero (waiter/waitress)**. Esto incluso a pesar de que BERT fue entrenado en un corpus supuestamente "neutral" de datos ([Wikipedia](https://huggingface.co/datasets/wikipedia) y [BookCorpus](https://huggingface.co/datasets/bookcorpus))

> Hay que ser consciente de que el modelo puede generar respuestas sexistas, homof√≥bicas o xenof√≥bicas sin tener nosotros tales intenciones debido a los datos disponibles.


# Resumen lecci√≥n

*In this chapter, you saw how to approach different NLP tasks using the high-level pipeline() function from ü§ó Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser.*

*We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:*

| Model   |      Examples      |  Tasks |
|----------|:-------------:|------:|
| Encoder |  ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Sentence classification, named entity recognition, extractive question answering |
| Decoder |    CTRL, GPT, GPT-2, Transformer XL   |   Text generation |
| Encoder-decoder | BART, T5, Marian, mBART |    Summarization, translation, generative question answering |


