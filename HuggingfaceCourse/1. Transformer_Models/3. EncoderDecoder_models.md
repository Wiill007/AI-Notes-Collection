# Encoder models

> [Anterior](./2.%20How_do_transformers_work.md) |  | [Siguiente](./4.%20SequenceSequence_models.md)

Encoder models use only the encoder of a Transformer model. En cada etapa, las capas de atención pueden acceder a todas las palabras en la frase inicial. Se caracterizan por tener atención bidireccional y a menudo se los llama *auto-encoding models*

Ante un input textual un encoder devuelve un vector de números llamados características *"features"* **para cada palabra**. Todas las palabras de la secuencia inicial afectan a la representación numérica de cada otra palabra.

*Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.*

La idea principal es que los encoders son muy potentes para extraer vectores de números que contienen información valiosa de una secuencia.

Modelos representativos de los encoders:
- ALBERT
- BERT
- DistilBERT
- ELECTRA
- RoBERTa


# Decoder models
Decoder models use only the Decoder of a Transformer model. En cada etapa, las capas de atención solo pueden acceder a las palabras posicionadas *antes* en la frase. Por tanto, tienen atención unidireccional y a menudo se los llama *auto-regressive models*

Ante un input textual un Decoder devuelve un vector de números llamados características *"features"* de una secuencia inicial. 

Son muy buenos para tareas generativas de texto *(Natural Language Generation NLG)*

Modelos representativos de los Decoders:
- CTRL
- GPT
- GPT-2
- Transformer XL

