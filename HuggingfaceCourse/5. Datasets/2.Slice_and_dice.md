# Slicing and dicing our dataset

> [Anterior](./1.Load_Datasets.md) |  | [Siguiente](./3.Big_data.md)

## Create new columns
Podemos usar la funci√≥n *.map()* para automatizar la creaci√≥n de una nueva columna entera. Por ejemplo para a√±adir una columna que cuente el n√∫mero de palabras en un dataset de reviews:
```python
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```
```python
drug_dataset = drug_dataset.map(compute_review_length)
# Inspect the first training example
drug_dataset["train"][0]
```
```python
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

Adem√°s si luego queremos ordenar dependiendo de este campo nuevo introducido:
```python
drug_dataset["train"].sort("review_length")[:3]
```


## More extensive guide about *.map()*'s powers
[Huggingface NLP course](https://huggingface.co/learn/nlp-course/en/chapter5/3#the-map-methods-superpowers)


## Creating a validation set
*Although we have a test set we could use for evaluation, it‚Äôs a good practice to leave the test set untouched and create a separate validation set during development. Once you are happy with the performance of your models on the validation set, you can do a final sanity check on the test set. This process helps mitigate the risk that you‚Äôll overfit to the test set and deploy a model that fails on real-world data.*

*ü§ó Datasets provides a Dataset.train_test_split() function that is based on the famous functionality from scikit-learn. Let‚Äôs use it to split our training set into train and validation splits (we set the seed argument for reproducibility):*
```python
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
# Rename the default "test" split to "validation"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
# Add the "test" set to our `DatasetDict`
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```
```python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```


## Saving datasets to disk
| Data format | Function |
|---|---|
| Arrow | Dataset.save_to_disk() |
| CSV | Dataset.to_csv() |
| JSON | Dataset.to_json() |
For example:
    drug_dataset_clean.save_to_disk("drug-reviews")